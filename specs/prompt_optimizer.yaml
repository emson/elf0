version: "0.1"
description: "Evaluator-Optimizer workflow for improving coding prompts through iterative refinement"
runtime: "langgraph"

llms:
  generator:
    _type: "openai"
    model_name: "gpt-4.1-mini"
    temperature: 0.7
    params:
      max_tokens: 1000
      system_prompt: |
        You are an expert prompt engineer specializing in coding prompts.
        Your task is to improve the given prompt by:
        1. Making it more specific and clear
        2. Adding necessary context and constraints
        3. Structuring it for better code generation
        4. Including relevant examples or patterns
        Focus on making the prompt more effective for generating high-quality code.

  evaluator:
    _type: "openai"
    model_name: "gpt-4.1-mini"
    temperature: 0.3
    params:
      max_tokens: 1000
      system_prompt: |
        You are a prompt evaluation expert.
        Evaluate the improved prompt based on these criteria:
        1. Clarity: Is the prompt clear and unambiguous?
        2. Completeness: Does it include all necessary context?
        3. Structure: Is it well-organized and easy to follow?
        4. Specificity: Does it provide enough detail for good code generation?
        Rate each criterion from 1-5 and provide specific feedback for improvement.
        If any criterion scores below 4, the prompt needs further refinement.

retrievers: {}
memory: {}
functions: {}

workflow:
  _type: "evaluator_optimizer"
  max_iterations: 3  # Maximum number of iterations before forcing completion
  nodes:
    - id: "generate"
      kind: "agent"
      ref: "generator"
      stop: false

    - id: "evaluate"
      kind: "judge"
      ref: "evaluator"
      stop: false

    - id: "end"
      kind: "agent"
      ref: "generator"
      stop: true

  edges:
    - source: "generate"
      target: "evaluate"
      condition: "True"

    - source: "evaluate"
      target: "end"
      condition: "state.get('evaluation_score', 0) >= 4 or state.get('iteration_count', 0) >= 3"

    - source: "evaluate"
      target: "generate"
      condition: "state.get('evaluation_score', 0) < 4 and state.get('iteration_count', 0) < 3"

eval:
  metrics:
    - quality
    - iterations
  dataset_path: "data/prompt_improvement_test.jsonl" 