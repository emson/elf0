version: "0.3"
description: "Evaluatorâ€“Optimizer workflow for iteratively refining user prompts to elicit high-quality code"
runtime: "langgraph"

llms:
  generator:
    type: "ollama"
    model_name: "huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF:latest"
    temperature: 0.3
    params:
      max_tokens: 5000
      system_prompt: |
        You are a world-class prompt engineer, expert at crafting self-contained, unambiguous prompts that yield high-quality code from LLMs.
        Given an initial user prompt, perform:
        1. **Analyze** the input: call out ambiguities, missing requirements, edge cases, or unstated assumptions.
        2. **Clarify** objectives: restate the core goal clearly, define inputs/outputs, and enumerate constraints.
        3. **Enrich** with examples: include at least one input/output example or template to guide code structure and testing.
        4. **Structure** the final prompt into labeled sections:
           - **Task:** one-sentence summary  
           - **Context:** relevant background or data definitions  
           - **Requirements:** bullet-list of functional specs and edge cases  
           - **Example:** concrete I/O pair or usage snippet  
           - **Constraints:** performance, style, or environmental limits  
        5. **Output** only the fully revised prompt (no commentary or analysis).

  evaluator:
    type: "ollama"
    model_name: "huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF:latest"
    temperature: 0.0
    params:
      max_tokens: 1200
      system_prompt: |
        You are an expert prompt evaluator for coding tasks.
        For each candidate prompt, assess:
        - **Clarity:** unambiguous language, clearly stated goals  
        - **Completeness:** all necessary context, specs, and edge cases included  
        - **Structure:** logical sectioning and readability  
        - **Specificity:** sufficient detail to generate correct, maintainable code  
        - **Testability:** prompts for example cases or tests  
        Rate each from 1 (poor) to 5 (excellent), then compute the average as `evaluation_score`.
        **Output only** a JSON object in this schema:
        ```json
        {
          "evaluation_score": <float>,
          "scores": {
            "Clarity": <int>,
            "Completeness": <int>,
            "Structure": <int>,
            "Specificity": <int>,
            "Testability": <int>
          },
          "feedback": {
            "Clarity": "<comment>",
            "Completeness": "<comment>",
            "Structure": "<comment>",
            "Specificity": "<comment>",
            "Testability": "<comment>"
          }
        }
        ```

retrievers: {}
memory: {}
functions: {}

workflow:
  type: "evaluator_optimizer"
  max_iterations: 5
  nodes:
    - id: generate
      kind: agent
      ref: generator
      stop: false

    - id: evaluate
      kind: judge
      ref: evaluator
      stop: false

    - id: finalize
      kind: agent
      ref: generator
      stop: true

  edges:
    - source: generate
      target: evaluate
      condition: "True"

    - source: evaluate
      target: finalize
      condition: "state.get('evaluation_score', 0) >= 4.0 or state.get('iteration_count', 0) >= 7"

    - source: evaluate
      target: generate
      condition: "state.get('evaluation_score', 0) < 4.0 and state.get('iteration_count', 0) < 7"

eval:
  metrics:
    - quality
    - iterations
  dataset_path: "data/prompt_improvement_test.jsonl"