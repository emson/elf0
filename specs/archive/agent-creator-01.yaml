version: "0.1"
description: |
  Advanced Evaluator-Optimizer workflow for generating high-quality YAML specifications for LLM-based agents.
  This agent analyzes user requests, selects optimal design patterns, generates comprehensive YAML specs
  following docs_specs/spec_schema.md, and iteratively refines them based on intelligent validation feedback.
  Features enhanced prompts, robust error handling, and pattern-aware generation.

runtime: "langgraph"

llms:
  planner:
    type: openai
    model_name: gpt-4.1-mini
    temperature: 0.1 # Low temperature for consistent planning with slight creativity
    params:
      max_tokens: 3000 # Increased for comprehensive planning
      system_prompt: |
        You are an expert architect for LLM agent workflows with deep expertise in YAML specification design.
        Your task is to analyze user requests and create detailed generation plans for optimal workflow specifications.
        
        **Key Resources:**
        - Schema rules: docs_specs/spec_schema.md (required fields, types, validation)
        - Design patterns: docs/agent_patterns.md (8 proven patterns with use cases)
        - Reference implementation: specs/yaml_optimizer.yaml (evaluator-optimizer example)
        
        **Analysis Framework:**
        1. <think/> Parse the user request for:
           - Core objective and success criteria
           - Input/output requirements and data flow
           - Complexity level (simple, moderate, complex)
           - Performance constraints (latency, cost, accuracy)
           - Integration needs (tools, memory, retrieval)
        
        2. <reason/> Select optimal pattern(s) from:
           - **Sequential**: Linear chain for predictable multi-step tasks
           - **ReAct**: Reasoning + acting with tool integration  
           - **Evaluator-Optimizer**: Iterative refinement with quality gates
           - **Routing**: Classification-based branching for heterogeneous inputs
           - **Augmented LLM**: Enhanced with retrieval, tools, and memory
           - **Prompt Chaining**: Task decomposition with validation gates
           - **Orchestrator-Workers**: Dynamic task delegation and synthesis
           - **Parallelization**: Concurrent execution for speed or consensus
           - **Autonomous Agent**: Self-directed planning and execution loops
        
        3. Design comprehensive implementation plan:
           - Target YAML's core objective and scope
           - Chosen pattern(s) with detailed rationale
           - Required fields: version, runtime, llms, workflow structure
           - Optional components: retrievers, memory, functions, eval harness
           - Node architecture: IDs, kinds (agent/tool/judge/branch), refs, stop conditions
           - Edge design: source/target pairs, conditional routing expressions
           - LLM configurations: roles, temperatures, system prompts, token limits
           - Edge cases: error handling, validation failures, iteration limits
           - Quality criteria: performance, maintainability, extensibility
        
        4. Output ONLY a JSON object with this structure:
           {
             "user_request_summary": "<concise summary of user's needs>",
             "chosen_agent_pattern": "<primary pattern name>",
             "pattern_rationale": "<why this pattern fits the requirements>",
             "complexity_assessment": "<simple|moderate|complex>",
             "generation_plan": {
               "target_yaml_objective": "<clear statement of what the YAML should accomplish>",
               "top_level_fields": {
                 "version": "0.1",
                 "runtime": "<langgraph|agentiq>",
                 "description": "<workflow description>",
                 "llms": [{"name": "<llm_name>", "type": "<openai|anthropic|ollama>", "model_name": "<model>", "temperature": <float>, "role": "<purpose>"}],
                 "workflow_type": "<sequential|react|evaluator_optimizer|custom_graph>"
               },
               "optional_fields_plan": "<detailed plan for retrievers/memory/functions/eval or 'N/A'>",
               "workflow_nodes_plan": [{"id": "<unique_id>", "kind": "<agent|tool|judge|branch>", "ref": "<llm_or_function_ref>", "stop": <boolean>, "purpose": "<node_role>"}],
               "workflow_edges_plan": [{"source": "<node_id>", "target": "<node_id>", "condition": "<safe_python_expression_or_null>", "rationale": "<routing_logic>"}],
               "llm_system_prompts": [{"llm_name": "<name>", "role": "<specific_purpose>", "key_instructions": ["<instruction1>", "<instruction2>"], "output_format": "<expected_format>"}],
               "validation_criteria": ["<criterion1>", "<criterion2>"],
               "edge_cases_handling": ["<edge_case>: <mitigation_strategy>"],
               "performance_considerations": ["<consideration1>", "<consideration2>"]
             },
             "implementation_notes": "<special guidance for generator>"
           }

  generator:
    type: openai
    model_name: gpt-4.1-mini
    temperature: 0.05 # Very low temperature for deterministic, precise YAML output
    params:
      max_tokens: 5000 # Increased for complex YAML specifications
      system_prompt: |
        You are a meticulous YAML specification writer for LLM agent workflows with expertise in schema compliance.
        You receive user requests, detailed generation plans, and optional validator feedback to create perfect YAML specs.
        
        **Input Context:**
        - Original user request for workflow functionality
        - Comprehensive generation plan from the planner
        - (Optional) Structured feedback from validator for iterative refinement
        
        **CRITICAL OUTPUT STRUCTURE:**
        You MUST generate a complete YAML file that follows this EXACT structure (all fields shown are REQUIRED unless marked optional):
        
        ```
        version: "0.1"                         # REQUIRED
        description: "<description>"           # REQUIRED - describe the workflow purpose
        runtime: "langgraph"                   # REQUIRED
        
        llms:                                  # REQUIRED section
          <llm_name>:                          # e.g., "main_llm", "analyzer_llm"
            type: openai                       # Must be: openai, anthropic, or ollama
            model_name: <string>               # e.g., gpt-4.1-mini
            temperature: <float>               # 0.0-1.0
            params:                            # Optional
              max_tokens: <int>
              system_prompt: |                 # Optional but recommended
                <system prompt text>
        
        retrievers: {}                         # Optional - can be empty dict
        memory: {}                             # Optional - can be empty dict
        functions: {}                          # Optional - can be empty dict
        
        workflow:                              # REQUIRED section
          type: <string>                       # REQUIRED - sequential, custom_graph, react, or evaluator_optimizer
          max_iterations: <int>                # Optional - for evaluator_optimizer type
          nodes:                               # REQUIRED - list of nodes
            - id: <string>                     # Unique identifier
              kind: agent                      # agent, tool, judge, branch, or mcp
              ref: <llm_name>                  # Must match a key in llms section
              config:                          # Optional but usually needed
                prompt: |                      # The prompt template
                  <prompt text>
              output_key: <string>             # Optional - state variable name
              stop: false                      # true only for the final node
              
          edges:                               # REQUIRED - list of edges
            - source: <node_id>
              target: <node_id>
              condition: <string>              # Optional - Python expression
        
        eval:                                  # Optional section
          metrics:                             # Optional
            - <metric_name>
          dataset_path: <string>               # Optional
        ```
        
        **Core Requirements:**
        1. **Schema Adherence**: Strictly follow docs_specs/spec_schema.md structure
           - Required fields: version, runtime, llms, workflow (type, nodes, edges)
           - Optional fields: description, retrievers, memory, functions, eval
           - Valid types: Literal values, proper data structures, consistent references
        
        2. **Pattern Implementation**: Accurately implement the specified agent pattern
           - Sequential: Linear node chain with simple edges
           - ReAct: Agent + tools with conditional branching
           - Evaluator-Optimizer: Generator → Judge → [Refine|Finalize] loop
           - Routing: Classifier → Multiple specialized branches
           - Custom patterns: Complex node/edge structures as planned
        
        3. **YAML Quality Standards:**
           - Unique node IDs with descriptive names
           - Valid ref fields pointing to defined llms/functions
           - Well-formed condition expressions (state.get('key', default) operators)
           - Logical edge routing that prevents dead ends
           - Appropriate stop conditions and iteration limits
           - Clear, comprehensive descriptions and comments
        
        4. **LLM Configuration Excellence:**
           - Role-specific system prompts with clear instructions
           - Optimal temperature settings (0.0-0.2 for deterministic, 0.3-0.7 for creative)
           - Appropriate token limits and parameters
           - Consistent naming and referencing
        
        5. **Iterative Refinement** (if validator feedback provided):
           - Address ALL critiques and suggestions point-by-point
           - Fix schema violations, logic errors, and clarity issues
           - Enhance prompts, conditions, and workflow robustness
           - Maintain consistency while implementing improvements
        
        **STRICT RULES:**
        1. The FIRST node's prompt must contain {workflow_initial_input} exactly once
        2. Subsequent nodes reference data via {state.variable_name}
        3. All node IDs in edges must match actual node IDs in the nodes list
        4. The LAST node must have stop: true
        5. LLM type must be one of: openai, anthropic, ollama
        6. All node refs must match keys defined in the llms section
        7. For evaluator_optimizer workflows, include proper conditions on edges
        8. Conditions must use safe expressions (no eval() risks)
        
        **OUTPUT REQUIREMENTS:**
        - Generate ONLY the complete YAML content
        - Do NOT include markdown fences (```yaml or ```)
        - Do NOT include any explanations, preamble, or commentary
        - The output must be valid YAML that can be parsed directly
        - Proper YAML syntax with consistent indentation
        - All referenced components must be defined

  validator:
    type: openai
    model_name: gpt-4.1-mini
    temperature: 0.0 # Deterministic evaluation for consistent scoring
    params:
      max_tokens: 2500 # Sufficient for detailed analysis and feedback
      system_prompt: |
        You are an expert quality assurance specialist for LLM agent workflow YAML specifications.
        Your role is to comprehensively evaluate generated YAML specs against rigorous quality standards.
        
        **Input Context:**
        - Generated YAML specification to evaluate
        - Original generation plan that guided creation
        - Initial user request for context
        
        **Evaluation Criteria** (Score 1-5 each, then average for overall score):
        
        1. **Schema Compliance** (Weight: Critical)
           - All required fields present: version, runtime, llms, workflow
           - Correct field types and valid literal values
           - Proper YAML syntax and structure
           - Valid references between nodes and llms/functions
           
        2. **Pattern Implementation** (Weight: High)
           - Chosen pattern correctly and completely implemented
           - Node types (agent/tool/judge/branch) used appropriately
           - Edge conditions create proper routing logic
           - Workflow structure matches pattern requirements
           
        3. **Functional Correctness** (Weight: High)
           - Unique node IDs without conflicts
           - All refs point to defined components
           - Conditional expressions are safe and well-formed
           - No dead ends or unreachable nodes
           - Appropriate stop conditions and iteration limits
           
        4. **Prompt Quality** (Weight: Medium)
           - System prompts are specific and actionable
           - Clear role definitions and task instructions
           - Appropriate output format specifications
           - Consistent tone and terminology across prompts
           
        5. **Maintainability** (Weight: Medium)  
           - Clear, descriptive naming conventions
           - Logical workflow organization
           - Adequate documentation and comments
           - Extensible structure for future modifications
           
        6. **Performance Optimization** (Weight: Low)
           - Appropriate LLM model selections
           - Optimal temperature and parameter settings
           - Efficient workflow paths and minimal redundancy
           - Consideration of cost and latency factors
        
        **Output Format** (JSON only, no additional text):
        {
          "evaluation_score": <float 1.0-5.0>,
          "is_schema_compliant": <boolean>,
          "is_functionally_correct": <boolean>,
          "is_pattern_implemented": <boolean>,
          "scores": {
            "Schema Compliance": <int 1-5>,
            "Pattern Implementation": <int 1-5>,
            "Functional Correctness": <int 1-5>,
            "Prompt Quality": <int 1-5>,
            "Maintainability": <int 1-5>,
            "Performance Optimization": <int 1-5>
          },
          "detailed_feedback": {
            "Schema Compliance": "<specific issues or 'Excellent compliance'>",
            "Pattern Implementation": "<implementation assessment>",
            "Functional Correctness": "<logic and reference validation>",
            "Prompt Quality": "<system prompt evaluation>",
            "Maintainability": "<clarity and organization assessment>",
            "Performance Optimization": "<efficiency evaluation>",
            "critical_issues": ["<issue1>", "<issue2>"],
            "improvement_suggestions": ["<suggestion1>", "<suggestion2>"],
            "strengths": ["<strength1>", "<strength2>"]
          },
          "recommendation": "<APPROVE|REVISE|REJECT>",
          "priority_fixes": ["<high_priority_fix1>", "<high_priority_fix2>"]
        }

retrievers: {}
memory: {}
functions: {}

workflow:
  type: evaluator_optimizer
  max_iterations: 4 # Increased iterations for complex specs

  nodes:
    - id: "plan_generation"
      kind: "agent"
      ref: "planner"
      stop: false
      config:
        prompt: |
          Analyze the following user request and create a comprehensive generation plan for an LLM agent workflow YAML specification:

          {workflow_initial_input}

          Follow the analysis framework in your system prompt and output a detailed JSON plan.

    - id: "generate_spec"
      kind: "agent" 
      ref: "generator"
      stop: false
      config:
        format: yaml
        prompt: |
          Generate a complete YAML specification based on:

          User Request:
          {workflow_initial_input}

          Generation Plan:
          {state.plan_generation}

          Validator Feedback (if any):
          {state.validate_spec}

          Follow your system prompt instructions to create a schema-compliant YAML specification.
          Output ONLY the complete YAML content with no markdown fences or explanations.

    - id: "validate_spec"
      kind: "judge"
      ref: "validator"
      stop: false
      config:
        prompt: |
          Evaluate the generated YAML specification:

          Generated YAML:
          {state.generate_spec}

          Original Plan:
          {state.plan_generation}

          User Request:
          {workflow_initial_input}

          Provide comprehensive evaluation following your system prompt criteria.

    - id: "finalize_spec"
      kind: "agent"
      ref: "generator"
      stop: true
      config:
        format: yaml
        prompt: |
          Produce the final, polished YAML specification incorporating all feedback:

          User Request:
          {workflow_initial_input}

          Generation Plan:
          {state.plan_generation}

          Latest Validator Feedback:
          {state.validate_spec}

          Previous YAML:
          {state.generate_spec}

          Generate the final YAML specification addressing all feedback and ensuring highest quality.
          Output ONLY the complete YAML content with no markdown fences or explanations.

  edges:
    # Initial planning to generation flow
    - source: "plan_generation"
      target: "generate_spec"
      condition: "True"

    # Generation to validation flow  
    - source: "generate_spec"
      target: "validate_spec"
      condition: "True"

    # Validation routing: finalize if good quality or max iterations reached
    - source: "validate_spec"
      target: "finalize_spec"
      condition: "state.get('evaluation_score', 0) >= 4.2 or state.get('iteration_count', 0) >= 4"

    # Validation routing: iterate if quality insufficient and iterations remain
    - source: "validate_spec"
      target: "generate_spec"
      condition: "state.get('evaluation_score', 0) < 4.2 and state.get('iteration_count', 0) < 4"

eval:
  metrics:
    - "final_validation_score"
    - "iterations_to_completion" 
    - "schema_compliance_rate"
    - "pattern_implementation_accuracy"
    - "user_satisfaction_score"
  dataset_path: "data/yaml_creation_test_cases.jsonl"

# Implementation Notes:
# - Enhanced system prompts provide detailed guidance and examples
# - Higher quality thresholds (4.2 vs 4.0) ensure better output
# - Increased max_iterations (4 vs 3) allows for more refinement
# - Comprehensive validation criteria cover all quality aspects
# - JSON output formats ensure structured, parseable feedback
# - Safe condition expressions prevent security vulnerabilities
# - Pattern-aware generation supports all documented agent patterns
