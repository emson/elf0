version: "0.3"
description: "Enhanced Coder Evaluator–Optimizer workflow for iteratively producing high-quality, tested code"
runtime: "langgraph"

llms:
  generator:
    type: "ollama"
    model_name: "huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF:latest"
    temperature: 0.1
    params:
      max_tokens: 8000
      system_prompt: |
        You are a world-class software engineer with deep expertise in multiple languages.
        When generating code:
        1. **Restate** the problem in a concise comment block at the top.
        2. **Design** the solution with clear modular functions and/or classes.
        3. **Document** each function/class with precise docstrings (inputs, outputs, behavior).
        4. **Implement** clean, idiomatic code that follows language-specific style guides (e.g. PEP8).
        5. **Write comprehensive unit tests** (using pytest) covering normal cases, edge cases, and error conditions.
        6. **Ensure tests are runnable** and all pass when executed.
        **Output** only the complete code file(s), including implementation and tests—no extra explanation.

  evaluator:
    type: "ollama"
    model_name: "huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct-GGUF:latest"
    temperature: 0
    params:
      max_tokens: 1500
      system_prompt: |
        You are an expert code evaluator. For each submission:
        1. **Execute** the provided implementation and its unit tests.
        2. If any test fails, set `"evaluation_score": 0.0` and list the failures under feedback.
        3. Otherwise, evaluate on:
           - **Clarity**: readability, comments, and documentation
           - **Completeness**: fulfills all specified requirements
           - **Structure**: modularity and logical organization
           - **Effectiveness**: correctness, performance, and test coverage
        Assign each criterion an integer score from 1 (poor) to 5 (excellent), then compute the average as `evaluation_score`.
        **Output only** a JSON object in this schema:
        ```json
        {
          "evaluation_score": <float>,
          "scores": {
            "Clarity": <int>,
            "Completeness": <int>,
            "Structure": <int>,
            "Effectiveness": <int>
          },
          "feedback": {
            "Clarity": "<comment>",
            "Completeness": "<comment>",
            "Structure": "<comment>",
            "Effectiveness": "<comment>"
          }
        }
        ```

workflow:
  type: "evaluator_optimizer"
  max_iterations: 3
  nodes:
    - id: generate
      kind: agent
      ref: generator
      stop: false

    - id: evaluate
      kind: judge
      ref: evaluator
      stop: false

    - id: finalize
      kind: agent
      ref: generator
      stop: true

  edges:
    - source: generate
      target: evaluate
      condition: "True"

    - source: evaluate
      target: finalize
      condition: "state.get('evaluation_score', 0) >= 4.0 or state.get('iteration_count', 0) >= 3"

    - source: evaluate
      target: generate
      condition: "state.get('evaluation_score', 0) < 4.0 and state.get('iteration_count', 0) < 3"

eval:
  metrics:
    - quality
    - iterations
  dataset_path: "data/prompt_improvement_test.jsonl"